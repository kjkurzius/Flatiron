{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7948ce94",
   "metadata": {},
   "source": [
    "\n",
    "# UrbanTech Feedback Classification Lab (Standalone Notebook)\n",
    "\n",
    "This notebook is **self-contained** and includes all code directly (no references to other notebooks).\n",
    "It implements an end-to-end Naive Bayes classifier for UrbanTech user feedback:\n",
    "\n",
    "1) Data Loading & Exploration  \n",
    "2) Text Preprocessing  \n",
    "3) Feature Extraction (BoW & TF-IDF)  \n",
    "4) Modeling with Naive Bayes + Top Words  \n",
    "5) Evaluation (reports + confusion matrices)  \n",
    "6) Model Improvement (GridSearchCV)  \n",
    "7) Prediction Wrapper & Artifacts\n",
    "\n",
    "**CSV search order:** `/mnt/data/urban_feedback.csv`, `/Users/karlkurzius/Downloads/urban_feedback.csv`, `./urban_feedback.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a17d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# Step 0: Imports & utilities\n",
    "import os, re, json, string\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             precision_recall_fscore_support)\n",
    "import joblib\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "\n",
    "def _ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def save_fig(name: str):\n",
    "    outdir = _ensure_dir(\"./figures\")\n",
    "    path = os.path.join(outdir, name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=140)\n",
    "    print(f\"[Saved figure] {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcc42a",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Locate and load dataset\n",
    "csv_candidates = [\n",
    "    \"/mnt/data/urban_feedback.csv\",\n",
    "    \"/Users/karlkurzius/Downloads/urban_feedback.csv\",\n",
    "    \"./urban_feedback.csv\",\n",
    "]\n",
    "csv_path = None\n",
    "for p in csv_candidates:\n",
    "    if os.path.exists(p):\n",
    "        csv_path = p\n",
    "        break\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\"Dataset not found. Expected at /mnt/data/urban_feedback.csv or /Users/karlkurzius/Downloads/urban_feedback.csv or ./urban_feedback.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Auto-detect text and label columns\n",
    "text_col_candidates = [c for c in df.columns if c.lower() in [\"text\",\"message\",\"feedback\",\"comment\",\"body\",\"review\"] or \"text\" in c.lower() or \"feedback\" in c.lower() or \"message\" in c.lower()]\n",
    "label_col_candidates = [c for c in df.columns if c.lower() in [\"label\",\"category\",\"department\",\"target\",\"class\"] or \"category\" in c.lower() or \"label\" in c.lower() or \"dept\" in c.lower()]\n",
    "\n",
    "if not text_col_candidates:\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"O\"]\n",
    "    if not obj_cols:\n",
    "        raise ValueError(\"No obvious text column found; rename your text column to 'text' or similar.\")\n",
    "    text_col_candidates = obj_cols[:1]\n",
    "if not label_col_candidates:\n",
    "    label_col_candidates = [df.columns[-1]]\n",
    "\n",
    "TEXT_COL = text_col_candidates[0]\n",
    "LABEL_COL = label_col_candidates[0]\n",
    "\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "\n",
    "display(df.head(10))\n",
    "\n",
    "# Label distribution and message lengths\n",
    "label_counts = df[LABEL_COL].value_counts().sort_values(ascending=False)\n",
    "df[\"_char_len\"] = df[TEXT_COL].astype(str).str.len()\n",
    "df[\"_word_len\"] = df[TEXT_COL].astype(str).str.split().apply(len)\n",
    "\n",
    "summary_stats = pd.DataFrame({\n",
    "    \"count\": [len(df)],\n",
    "    \"unique_labels\": [df[LABEL_COL].nunique()],\n",
    "    \"avg_chars\": [df[\"_char_len\"].mean()],\n",
    "    \"median_chars\": [df[\"_char_len\"].median()],\n",
    "    \"avg_words\": [df[\"_word_len\"].mean()],\n",
    "    \"median_words\": [df[\"_word_len\"].median()],\n",
    "})\n",
    "display(summary_stats)\n",
    "display(label_counts.rename_axis(LABEL_COL).reset_index(name=\"count\"))\n",
    "\n",
    "# Visualizations\n",
    "plt.figure()\n",
    "plt.bar(label_counts.index.astype(str), label_counts.values)\n",
    "plt.title(\"Category Distribution\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "save_fig(\"category_distribution.png\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df[\"_word_len\"], bins=30)\n",
    "plt.title(\"Message Word Length Distribution\")\n",
    "plt.xlabel(\"Words per message\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "save_fig(\"message_word_length_hist.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce1ba5",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665716ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_PUNCT_TBL = str.maketrans(\"\", \"\", string.punctuation)\n",
    "DEFAULT_STOPWORDS = set(ENGLISH_STOP_WORDS) | {\n",
    "    \"app\",\"apps\",\"ut\",\"urbantech\",\"transit\",\"route\",\"routes\",\"train\",\"bus\",\"buses\"\n",
    "}\n",
    "TOKEN_PATTERN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "\n",
    "def naive_lemmatize(token: str) -> str:\n",
    "    t = token\n",
    "    if t.endswith(\"ies\") and len(t) > 4:\n",
    "        return t[:-3] + \"y\"\n",
    "    if t.endswith((\"sses\",\"shes\",\"ches\")):\n",
    "        return t[:-2]\n",
    "    if t.endswith(\"s\") and len(t) > 3 and not t.endswith(\"ss\"):\n",
    "        t = t[:-1]\n",
    "    if t.endswith(\"ing\") and len(t) > 5:\n",
    "        t = t[:-3]\n",
    "        if t.endswith(\"y\"):\n",
    "            t = t[:-1] + \"i\"\n",
    "    elif t.endswith(\"ed\") and len(t) > 4:\n",
    "        t = t[:-2]\n",
    "    return t\n",
    "\n",
    "def preprocess_text(text: str, stopwords: Optional[set]=None, lemmatize: bool=True) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(_PUNCT_TBL)\n",
    "    tokens = TOKEN_PATTERN.findall(text)\n",
    "    sw = DEFAULT_STOPWORDS if stopwords is None else stopwords\n",
    "    out: List[str] = []\n",
    "    for tok in tokens:\n",
    "        if tok in sw:\n",
    "            continue\n",
    "        tok2 = naive_lemmatize(tok) if lemmatize else tok\n",
    "        if tok2 and tok2 not in sw and tok2.isalpha():\n",
    "            out.append(tok2)\n",
    "    return \" \".join(out)\n",
    "\n",
    "# Preview preprocessing\n",
    "preview = pd.DataFrame({\n",
    "    \"original\": df[TEXT_COL].astype(str).head(8).tolist(),\n",
    "    \"preprocessed\": [preprocess_text(s) for s in df[TEXT_COL].astype(str).head(8)]\n",
    "})\n",
    "display(preview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb45efa",
   "metadata": {},
   "source": [
    "## Step 3: Feature Extraction (BoW & TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X = df[TEXT_COL].astype(str).apply(preprocess_text)\n",
    "y = df[LABEL_COL].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "def nonempty(series: pd.Series) -> pd.Series:\n",
    "    return series.apply(lambda s: s if isinstance(s, str) and s.strip() else \"placeholdertoken\")\n",
    "\n",
    "X_train_ne = nonempty(X_train)\n",
    "X_test_ne  = nonempty(X_test)\n",
    "\n",
    "bow_vectorizer = CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1,1),\n",
    "                                 lowercase=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=1.0, ngram_range=(1,1),\n",
    "                                   lowercase=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train_ne)\n",
    "X_test_bow  = bow_vectorizer.transform(X_test_ne)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_ne)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_ne)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa26e8d",
   "metadata": {},
   "source": [
    "## Step 4: Model Building with Naive Bayes + Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_bow = MultinomialNB(alpha=1.0).fit(X_train_bow, y_train)\n",
    "nb_tfidf = MultinomialNB(alpha=1.0).fit(X_train_tfidf, y_train)\n",
    "\n",
    "def top_words_per_class(vectorizer, clf: MultinomialNB, k=12) -> pd.DataFrame:\n",
    "    feats = np.array(vectorizer.get_feature_names_out())\n",
    "    rows = []\n",
    "    for i, cls in enumerate(clf.classes_):\n",
    "        lp = clf.feature_log_prob_[i]\n",
    "        idx = np.argsort(lp)[-k:][::-1]\n",
    "        for w, s in zip(feats[idx], lp[idx]):\n",
    "            rows.append({\"class\": cls, \"word\": w, \"log_prob\": float(s)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "display(top_words_per_class(bow_vectorizer, nb_bow, k=12).groupby(\"class\").head(12))\n",
    "display(top_words_per_class(tfidf_vectorizer, nb_tfidf, k=12).groupby(\"class\").head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6924091",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation (reports + confusion matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(clf, Xte, Yte, title: str, fig_name: str):\n",
    "    preds = clf.predict(Xte)\n",
    "    acc = accuracy_score(Yte, preds)\n",
    "    print(f\"\\n== {title} ==\\nAccuracy: {acc:.3f}\\n\")\n",
    "    print(classification_report(Yte, preds, digits=3))\n",
    "    labels = sorted(Yte.unique())\n",
    "    cm = confusion_matrix(Yte, preds, labels=labels)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(ticks=range(len(labels)), labels=labels, rotation=30, ha=\"right\")\n",
    "    plt.yticks(ticks=range(len(labels)), labels=labels)\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    save_fig(fig_name); plt.show()\n",
    "\n",
    "evaluate(nb_bow, X_test_bow, y_test, \"NB + BoW (baseline)\", \"cm_nb_bow_baseline.png\")\n",
    "evaluate(nb_tfidf, X_test_tfidf, y_test, \"NB + TF-IDF (baseline)\", \"cm_nb_tfidf_baseline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990bd96",
   "metadata": {},
   "source": [
    "## Step 6: Model Improvement (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aea008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pipeline(kind=\"bow\"):\n",
    "    vec = CountVectorizer(lowercase=False, token_pattern=r\"(?u)\\b\\w+\\b\") if kind==\"bow\" else TfidfVectorizer(lowercase=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    nb = MultinomialNB()\n",
    "    return Pipeline([(\"vec\", vec), (\"nb\", nb)])\n",
    "\n",
    "param_grid = {\n",
    "    \"vec__min_df\": [1, 2],\n",
    "    \"vec__max_df\": [0.9, 1.0],\n",
    "    \"vec__ngram_range\": [(1,1), (1,2)],\n",
    "    \"nb__alpha\": [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "pip_bow = build_pipeline(\"bow\")\n",
    "pip_tfidf = build_pipeline(\"tfidf\")\n",
    "\n",
    "gscv_bow = GridSearchCV(pip_bow, param_grid, cv=3, n_jobs=-1, scoring=\"f1_macro\")\n",
    "gscv_tfidf = GridSearchCV(pip_tfidf, param_grid, cv=3, n_jobs=-1, scoring=\"f1_macro\")\n",
    "\n",
    "gscv_bow.fit(X_train_ne, y_train)\n",
    "gscv_tfidf.fit(X_train_ne, y_train)\n",
    "\n",
    "best_bow = gscv_bow.best_estimator_\n",
    "best_tfidf = gscv_tfidf.best_estimator_\n",
    "\n",
    "print(\"Best BoW params:\", gscv_bow.best_params_)\n",
    "print(\"Best TF-IDF params:\", gscv_tfidf.best_params_)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "best_bow_preds = best_bow.predict(X_test_ne)\n",
    "best_tfidf_preds = best_tfidf.predict(X_test_ne)\n",
    "\n",
    "rep_bow_text = classification_report(y_test, best_bow_preds, digits=3)\n",
    "rep_tfidf_text = classification_report(y_test, best_tfidf_preds, digits=3)\n",
    "\n",
    "print(\"\\nBest BoW Classification Report:\\n\", rep_bow_text)\n",
    "print(\"\\nBest TF-IDF Classification Report:\\n\", rep_tfidf_text)\n",
    "\n",
    "# Choose best by macro-F1 parsed from the text table\n",
    "def macro_f1(report_text: str) -> float:\n",
    "    lines = [ln for ln in report_text.splitlines() if \"macro avg\" in ln]\n",
    "    if not lines: return 0.0\n",
    "    parts = [p for p in lines[0].split(\" \") if p.strip()]\n",
    "    try:\n",
    "        return float(parts[-2])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "f1_bow = macro_f1(rep_bow_text)\n",
    "f1_tfidf = macro_f1(rep_tfidf_text)\n",
    "\n",
    "chosen = \"TF-IDF\" if f1_tfidf >= f1_bow else \"BoW\"\n",
    "best_pipeline = best_tfidf if chosen==\"TF-IDF\" else best_bow\n",
    "print(f\"\\nChosen best model: {chosen}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b5d43",
   "metadata": {},
   "source": [
    "## Step 7: Prediction Wrapper & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UrbanTechFeedbackClassifier:\n",
    "    def __init__(self, pipeline: Pipeline):\n",
    "        self.pipeline = pipeline\n",
    "    def preprocess(self, texts: List[str]) -> List[str]:\n",
    "        return [preprocess_text(t) for t in texts]\n",
    "    def predict(self, texts: List[str]):\n",
    "        clean = self.preprocess(texts)\n",
    "        clean_ne = [c if c.strip() else \"placeholdertoken\" for c in clean]\n",
    "        probs = self.pipeline.predict_proba(clean_ne)\n",
    "        preds = self.pipeline.predict(clean_ne)\n",
    "        out = []\n",
    "        for t, p, pr in zip(texts, probs, preds):\n",
    "            out.append({\n",
    "                \"text\": t,\n",
    "                \"predicted_label\": pr,\n",
    "                \"confidence\": float(np.max(p)),\n",
    "                \"probabilities\": {cls: float(prob) for cls, prob in zip(self.pipeline.named_steps[\"nb\"].classes_, p)}\n",
    "            })\n",
    "        return out\n",
    "\n",
    "wrapper = UrbanTechFeedbackClassifier(best_pipeline)\n",
    "\n",
    "examples = [\n",
    "    \"The map sent me to the wrong platform and the transfer time was off.\",\n",
    "    \"The app keeps freezing when I try to buy a ticket.\",\n",
    "    \"The train delay information was outdated by 20 minutes.\"\n",
    "]\n",
    "pd.DataFrame(wrapper.predict(examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save artifacts\n",
    "joblib.dump(best_pipeline, \"/mnt/data/urbantech_best_pipeline.joblib\")\n",
    "with open(\"/mnt/data/urbantech_metadata.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_model_type\": chosen,\n",
    "        \"best_params\": best_pipeline.get_params()\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Saved: /mnt/data/urbantech_best_pipeline.joblib\")\n",
    "print(\"Saved: /mnt/data/urbantech_metadata.json\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
