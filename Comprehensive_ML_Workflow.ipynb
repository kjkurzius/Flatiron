{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7310bb38",
   "metadata": {},
   "source": [
    "# Comprehensive Multiâ€‘Dataset ML Workflow\n",
    "Reproducible pipeline with artifact logging (plots/tables/metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c22aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Comprehensive multi-dataset ML workflow with reproducibility and artifact logging.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Mapping, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.figure import Figure\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, calinski_harabasz_score, classification_report\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "RANDOM_STATE: int = 42\n",
    "ARTIFACT_DIR = Path(\"artifacts\")\n",
    "PLOT_DIR = ARTIFACT_DIR / \"plots\"\n",
    "TABLE_DIR = ARTIFACT_DIR / \"tables\"\n",
    "METRIC_DIR = ARTIFACT_DIR / \"metrics\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassificationResult:\n",
    "    \"\"\"Container for wine classification outputs.\"\"\"\n",
    "\n",
    "    accuracy: float\n",
    "    classification_report: str\n",
    "    best_params: Mapping[str, object]\n",
    "    label_encoder_classes: Sequence[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RecommendationResult:\n",
    "    \"\"\"Container for feed recommendation embeddings and similarities.\"\"\"\n",
    "\n",
    "    feed_embeddings: pd.DataFrame\n",
    "    cosine_similarity: pd.DataFrame\n",
    "    top_recommendations: Mapping[str, Mapping[str, float]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClusteringResult:\n",
    "    \"\"\"Container for clustering model outputs and diagnostics.\"\"\"\n",
    "\n",
    "    kmeans_labels: np.ndarray\n",
    "    gmm_labels: np.ndarray\n",
    "    gmm_probabilities: np.ndarray\n",
    "    silhouette_scores: Dict[str, float]\n",
    "    metric_table: pd.DataFrame\n",
    "\n",
    "\n",
    "def _resolve_dataset_path(mac_path: str, workspace_path: str) -> Path:\n",
    "    \"\"\"Resolve dataset path, preferring the local user path but falling back to workspace.\"\"\"\n",
    "\n",
    "    for candidate in (Path(mac_path), Path(workspace_path)):\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"None of the provided paths exist: {mac_path!r}, {workspace_path!r}\")\n",
    "\n",
    "\n",
    "def _ensure_directories(*directories: Path) -> None:\n",
    "    \"\"\"Create artifact directories if they do not already exist.\"\"\"\n",
    "\n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def summarize_dataframe(name: str, df: pd.DataFrame) -> Dict[str, object]:\n",
    "    \"\"\"Return a reproducible summary for logging and auditing.\"\"\"\n",
    "\n",
    "    summary = {\n",
    "        \"name\": name,\n",
    "        \"shape\": df.shape,\n",
    "        \"null_counts\": df.isna().sum().to_dict(),\n",
    "    }\n",
    "    logging.info(\"%s summary: %s\", name, summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fill missing values with sensible defaults to handle inconsistencies.\"\"\"\n",
    "\n",
    "    cleaned = df.copy()\n",
    "    for column in cleaned.columns:\n",
    "        if cleaned[column].isna().any():\n",
    "            if cleaned[column].dtype.kind in \"biufc\":\n",
    "                cleaned[column] = cleaned[column].fillna(cleaned[column].mean())\n",
    "            else:\n",
    "                mode = cleaned[column].mode(dropna=True)\n",
    "                fill_value = mode.iloc[0] if not mode.empty else \"unknown\"\n",
    "                cleaned[column] = cleaned[column].fillna(fill_value)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def _save_plot(fig: Figure, filename: str) -> None:\n",
    "    \"\"\"Persist a Matplotlib figure to disk and close it to free memory.\"\"\"\n",
    "\n",
    "    filepath = PLOT_DIR / filename\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(filepath, dpi=200)\n",
    "    plt.close(fig)\n",
    "    logging.info(\"Saved plot to %s\", filepath)\n",
    "\n",
    "\n",
    "def _save_table(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Save a DataFrame to disk as CSV for downstream inspection.\"\"\"\n",
    "\n",
    "    filepath = TABLE_DIR / filename\n",
    "    df.to_csv(filepath, index=True)\n",
    "    logging.info(\"Saved table to %s\", filepath)\n",
    "\n",
    "\n",
    "def _save_metrics(metrics: Mapping[str, object], filename: str) -> None:\n",
    "    \"\"\"Persist JSON metrics to disk.\"\"\"\n",
    "\n",
    "    filepath = METRIC_DIR / filename\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(metrics, fh, indent=2)\n",
    "    logging.info(\"Saved metrics to %s\", filepath)\n",
    "\n",
    "\n",
    "def _encode_target(target: pd.Series) -> Tuple[pd.Series, Sequence[str]]:\n",
    "    \"\"\"Convert categorical target labels into numeric values using LabelEncoder.\"\"\"\n",
    "\n",
    "    if target.dtype.kind in \"biufc\":\n",
    "        return target, []\n",
    "    encoder = LabelEncoder()\n",
    "    encoded = encoder.fit_transform(target)\n",
    "    return pd.Series(encoded, index=target.index, name=target.name), encoder.classes_\n",
    "\n",
    "\n",
    "def run_wine_classification(df: pd.DataFrame) -> ClassificationResult:\n",
    "    \"\"\"Train and evaluate a PCA + k-NN pipeline on the wine dataset.\"\"\"\n",
    "\n",
    "    features = df.drop(columns=[\"target\"]).copy()\n",
    "    target, classes = _encode_target(df[\"target\"].copy())\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        target,\n",
    "        test_size=0.25,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=target,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"pca\", PCA(n_components=0.95, random_state=RANDOM_STATE)),\n",
    "            (\"knn\", KNeighborsClassifier()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        \"knn__n_neighbors\": [1, 3, 5, 7, 9, 11],\n",
    "        \"knn__metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    }\n",
    "\n",
    "    splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=splitter,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    predictions = grid.best_estimator_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions)\n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"cv_mean_accuracy\": float(grid.best_score_),\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"classification_report\": report_dict,\n",
    "        \"label_classes\": list(classes),\n",
    "    }\n",
    "    _save_metrics(metrics, \"wine_classification.json\")\n",
    "\n",
    "    return ClassificationResult(\n",
    "        accuracy=accuracy,\n",
    "        classification_report=report,\n",
    "        best_params=grid.best_params_,\n",
    "        label_encoder_classes=classes,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_feed_recommendations(df: pd.DataFrame) -> RecommendationResult:\n",
    "    \"\"\"Standardize feed weights, project to 1D with PCA, and compute cosine similarities.\"\"\"\n",
    "\n",
    "    if \"feed_name\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'feed_name' column in chick feed dataset.\")\n",
    "\n",
    "    numeric_columns = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not numeric_columns:\n",
    "        raise ValueError(\"Expected at least one numeric column for recommendations.\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized = scaler.fit_transform(df[numeric_columns])\n",
    "    standardized_df = pd.DataFrame(standardized, columns=numeric_columns)\n",
    "    standardized_df[\"feed_name\"] = df[\"feed_name\"].values\n",
    "\n",
    "    feed_profiles = standardized_df.groupby(\"feed_name\").mean()\n",
    "\n",
    "    pca = PCA(n_components=1, random_state=RANDOM_STATE)\n",
    "    embeddings = pca.fit_transform(feed_profiles.values)\n",
    "    embed_df = pd.DataFrame(embeddings, index=feed_profiles.index, columns=[\"pc1\"])\n",
    "\n",
    "    cosine = cosine_similarity(embed_df)\n",
    "    cosine_df = pd.DataFrame(cosine, index=feed_profiles.index, columns=feed_profiles.index)\n",
    "\n",
    "    recommendations: Dict[str, Dict[str, float]] = {}\n",
    "    for feed in cosine_df.index:\n",
    "        top_similar = (\n",
    "            cosine_df.loc[feed].drop(feed).sort_values(ascending=False).head(3).to_dict()\n",
    "        )\n",
    "        recommendations[feed] = top_similar\n",
    "\n",
    "    _save_table(embed_df, \"feed_embeddings.csv\")\n",
    "    _save_table(cosine_df, \"feed_cosine_similarity.csv\")\n",
    "    _save_metrics(recommendations, \"feed_recommendations.json\")\n",
    "\n",
    "    return RecommendationResult(\n",
    "        feed_embeddings=embed_df,\n",
    "        cosine_similarity=cosine_df,\n",
    "        top_recommendations=recommendations,\n",
    "    )\n",
    "\n",
    "\n",
    "def _compute_line_distance(points: np.ndarray) -> int:\n",
    "    \"\"\"Identify the elbow point as the maximum distance from the straight line between endpoints.\"\"\"\n",
    "\n",
    "    start, end = points[0], points[-1]\n",
    "    line_vec = end - start\n",
    "    norm = np.linalg.norm(line_vec)\n",
    "    if norm == 0:\n",
    "        return 0\n",
    "    line_norm = line_vec / norm\n",
    "    distances = []\n",
    "    for point in points:\n",
    "        projection_length = np.dot(point - start, line_norm)\n",
    "        projection = start + projection_length * line_norm\n",
    "        distances.append(np.linalg.norm(point - projection))\n",
    "    return int(np.argmax(distances))\n",
    "\n",
    "\n",
    "def _select_k_via_elbow(inertias: Sequence[float], ks: Sequence[int]) -> int:\n",
    "    \"\"\"Select the number of clusters using the elbow heuristic with a distance-based method.\"\"\"\n",
    "\n",
    "    points = np.column_stack((ks, inertias))\n",
    "    idx = _compute_line_distance(points)\n",
    "    selected_k = ks[idx]\n",
    "    if selected_k < 2 and len(ks) > 1:\n",
    "        selected_k = ks[1]\n",
    "    return selected_k\n",
    "\n",
    "\n",
    "def _evaluate_cluster_metrics(X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute a suite of clustering metrics for the given labels.\"\"\"\n",
    "\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return {\"silhouette\": float(\"nan\"), \"calinski_harabasz\": float(\"nan\"), \"davies_bouldin\": float(\"nan\")}\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    calinski = calinski_harabasz_score(X, labels)\n",
    "    davies = davies_bouldin_score(X, labels)\n",
    "    return {\n",
    "        \"silhouette\": silhouette,\n",
    "        \"calinski_harabasz\": calinski,\n",
    "        \"davies_bouldin\": davies,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_arrests_clustering(df: pd.DataFrame) -> ClusteringResult:\n",
    "    \"\"\"Cluster the USArrests dataset with multiple diagnostics and artifact logging.\"\"\"\n",
    "\n",
    "    features = [\"Murder\", \"Assault\", \"UrbanPop\", \"Rape\"]\n",
    "    variance_rank = df[features].var().sort_values(ascending=False)\n",
    "    top_features = variance_rank.index.tolist()[:3]\n",
    "    logging.info(\"Top variance features: %s\", top_features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized = scaler.fit_transform(df[top_features])\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "    embeddings = pca.fit_transform(standardized)\n",
    "\n",
    "    ks = list(range(2, 10))\n",
    "    inertia_values: List[float] = []\n",
    "    silhouette_values: List[float] = []\n",
    "    calinski_values: List[float] = []\n",
    "    davies_values: List[float] = []\n",
    "\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=20, random_state=RANDOM_STATE)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        inertia_values.append(kmeans.inertia_)\n",
    "        metrics = _evaluate_cluster_metrics(embeddings, labels)\n",
    "        silhouette_values.append(metrics[\"silhouette\"])\n",
    "        calinski_values.append(metrics[\"calinski_harabasz\"])\n",
    "        davies_values.append(metrics[\"davies_bouldin\"])\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            \"k\": ks,\n",
    "            \"inertia\": inertia_values,\n",
    "            \"silhouette\": silhouette_values,\n",
    "            \"calinski_harabasz\": calinski_values,\n",
    "            \"davies_bouldin\": davies_values,\n",
    "        }\n",
    "    )\n",
    "    _save_table(metrics_df, \"arrests_cluster_metrics.csv\")\n",
    "\n",
    "    elbow_k = _select_k_via_elbow(inertia_values, ks)\n",
    "    logging.info(\"Elbow-selected k: %s\", elbow_k)\n",
    "\n",
    "    bic_scores = []\n",
    "    for k in ks:\n",
    "        gmm = GaussianMixture(n_components=k, random_state=RANDOM_STATE, n_init=10)\n",
    "        gmm.fit(embeddings)\n",
    "        bic_scores.append(gmm.bic(embeddings))\n",
    "    bic_df = pd.DataFrame({\"k\": ks, \"bic\": bic_scores})\n",
    "    _save_table(bic_df, \"arrests_bic_scores.csv\")\n",
    "    gmm_k = int(bic_df.sort_values(\"bic\").iloc[0][\"k\"])\n",
    "\n",
    "    kmeans_final = KMeans(n_clusters=elbow_k, n_init=20, random_state=RANDOM_STATE).fit(embeddings)\n",
    "    gmm_final = GaussianMixture(n_components=gmm_k, random_state=RANDOM_STATE, n_init=10).fit(embeddings)\n",
    "\n",
    "    kmeans_labels = kmeans_final.predict(embeddings)\n",
    "    gmm_labels = gmm_final.predict(embeddings)\n",
    "    gmm_probabilities = gmm_final.predict_proba(embeddings).max(axis=1)\n",
    "\n",
    "    kmeans_metrics = _evaluate_cluster_metrics(embeddings, kmeans_labels)\n",
    "    gmm_metrics = _evaluate_cluster_metrics(embeddings, gmm_labels)\n",
    "    silhouette_scores = {\n",
    "        \"kmeans\": kmeans_metrics.get(\"silhouette\", float(\"nan\")),\n",
    "        \"gmm\": gmm_metrics.get(\"silhouette\", float(\"nan\")),\n",
    "    }\n",
    "    _save_metrics(\n",
    "        {\n",
    "            \"silhouette\": silhouette_scores,\n",
    "            \"elbow_k\": elbow_k,\n",
    "            \"gmm_k\": gmm_k,\n",
    "            \"kmeans_metrics\": kmeans_metrics,\n",
    "            \"gmm_metrics\": gmm_metrics,\n",
    "        },\n",
    "        \"arrests_clustering.json\",\n",
    "    )\n",
    "\n",
    "    cluster_table = df[[\"State\"]].copy()\n",
    "    cluster_table[\"kmeans_cluster\"] = kmeans_labels\n",
    "    cluster_table[\"gmm_cluster\"] = gmm_labels\n",
    "    cluster_table[\"gmm_probability\"] = gmm_probabilities\n",
    "    _save_table(cluster_table, \"arrests_clusters.csv\")\n",
    "\n",
    "    fig_elbow, ax_elbow = plt.subplots(figsize=(6, 4))\n",
    "    ax_elbow.plot(ks, inertia_values, marker=\"o\")\n",
    "    ax_elbow.axvline(elbow_k, color=\"red\", linestyle=\"--\", label=f\"Elbow k={elbow_k}\")\n",
    "    ax_elbow.set_title(\"K-Means Elbow Plot\")\n",
    "    ax_elbow.set_xlabel(\"Number of clusters (k)\")\n",
    "    ax_elbow.set_ylabel(\"WCSS\")\n",
    "    ax_elbow.legend()\n",
    "    _save_plot(fig_elbow, \"arrests_elbow.png\")\n",
    "\n",
    "    fig_bic, ax_bic = plt.subplots(figsize=(6, 4))\n",
    "    ax_bic.plot(ks, bic_scores, marker=\"o\")\n",
    "    ax_bic.axvline(gmm_k, color=\"red\", linestyle=\"--\", label=f\"BIC k={gmm_k}\")\n",
    "    ax_bic.set_title(\"GMM BIC Scores\")\n",
    "    ax_bic.set_xlabel(\"Number of components (k)\")\n",
    "    ax_bic.set_ylabel(\"BIC\")\n",
    "    ax_bic.legend()\n",
    "    _save_plot(fig_bic, \"arrests_bic.png\")\n",
    "\n",
    "    fig_clusters, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "    scatter_kwargs = dict(cmap=\"viridis\", edgecolor=\"k\")\n",
    "    axes[0].scatter(embeddings[:, 0], embeddings[:, 1], c=kmeans_labels, **scatter_kwargs)\n",
    "    axes[0].set_title(f\"K-Means Clusters (k={elbow_k})\")\n",
    "    axes[0].set_xlabel(\"PC1\")\n",
    "    axes[0].set_ylabel(\"PC2\")\n",
    "    axes[1].scatter(embeddings[:, 0], embeddings[:, 1], c=gmm_labels, **scatter_kwargs)\n",
    "    axes[1].set_title(f\"GMM Clusters (k={gmm_k})\")\n",
    "    axes[1].set_xlabel(\"PC1\")\n",
    "    _save_plot(fig_clusters, \"arrests_cluster_scatter.png\")\n",
    "\n",
    "    return ClusteringResult(\n",
    "        kmeans_labels=kmeans_labels,\n",
    "        gmm_labels=gmm_labels,\n",
    "        gmm_probabilities=gmm_probabilities,\n",
    "        silhouette_scores=silhouette_scores,\n",
    "        metric_table=metrics_df,\n",
    "    )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Entry point for executing the workflow end-to-end.\"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "\n",
    "    _ensure_directories(ARTIFACT_DIR, PLOT_DIR, TABLE_DIR, METRIC_DIR)\n",
    "\n",
    "    wine_path = _resolve_dataset_path(\n",
    "        \"/Users/karlkurzius/Downloads/wine_data.csv\",\n",
    "        \"/mnt/data/wine_data.csv\",\n",
    "    )\n",
    "    chick_path = _resolve_dataset_path(\n",
    "        \"/Users/karlkurzius/Downloads/chickwts_data.csv\",\n",
    "        \"/mnt/data/chickwts_data.csv\",\n",
    "    )\n",
    "    arrest_path = _resolve_dataset_path(\n",
    "        \"/Users/karlkurzius/Downloads/arrest_data.csv\",\n",
    "        \"/mnt/data/arrests_data.csv\",\n",
    "    )\n",
    "\n",
    "    wine_df = clean_dataframe(pd.read_csv(wine_path))\n",
    "    chick_df = clean_dataframe(pd.read_csv(chick_path))\n",
    "    arrest_df = clean_dataframe(pd.read_csv(arrest_path).rename(columns={\"Unnamed: 0\": \"State\"}))\n",
    "\n",
    "    summaries = {\n",
    "        \"wine\": summarize_dataframe(\"wine\", wine_df),\n",
    "        \"chickwts\": summarize_dataframe(\"chickwts\", chick_df),\n",
    "        \"us_arrests\": summarize_dataframe(\"us_arrests\", arrest_df),\n",
    "    }\n",
    "    _save_metrics(summaries, \"dataset_summaries.json\")\n",
    "\n",
    "    wine_result = run_wine_classification(wine_df)\n",
    "    logging.info(\"Wine accuracy: %.3f\", wine_result.accuracy)\n",
    "    logging.info(\"Wine best params: %s\", wine_result.best_params)\n",
    "\n",
    "    feed_result = run_feed_recommendations(chick_df)\n",
    "    logging.info(\"Feed cosine similarity matrix saved with shape %s\", feed_result.cosine_similarity.shape)\n",
    "\n",
    "    arrests_result = run_arrests_clustering(arrest_df)\n",
    "    logging.info(\"KMeans silhouette: %.3f\", arrests_result.silhouette_scores[\"kmeans\"])\n",
    "    logging.info(\"GMM silhouette: %.3f\", arrests_result.silhouette_scores[\"gmm\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
